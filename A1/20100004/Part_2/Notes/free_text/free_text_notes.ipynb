{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREAMBLE\n",
    "# Free text processing\n",
    "# text.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Text\n",
    "\n",
    "This set of notes focuses on processing free text data.  In addition to the more structured relational data and graphs we have discussed previously, free text makes up one of the most common types of \"widely available\" data: web pages, unstructured \"comment\" fields in many relational databases, and many other easily-obtained large sources of data naturally come in free text form.  The notable difference, of course ,is that unlike the data types we have discussed before, free text lacks the \"easily extractable\" structure inherent in the previous types of data we have considered.\n",
    "\n",
    "This is not to say, of course, that free text data lacks structure.  Just the opposite: by its very definition free text usually needs to have meaning to the people who are reading that data.  But the task of actually understanding this structure is a problem that is still beyond the scope of \"generic\" data science tools, because it often involves human-level intelligence (or at least bringing to bear an enormous amount of external context) to understand the meaning behind the text.  \n",
    "\n",
    "This is also naturally a debatable point, and this particular perspective on free text data is one that may need to be reconsidered based upon advances in the upcoming years.  For instance, we will later in the course deal with algorithms for processing image data for data science tasks, but only after we have (briefly, at a very high level) covered some methods for deep learning; indeed, handling images will be one of the primary use cases for deep learning in data science.  And there are already some instances of these techniques that appear in this lecture too: the word2vec model (which is an addition to the lecture this year), _is_ in fact a neural network model that we will use to understand text data.\n",
    "\n",
    "But the primary difference is that, in my opinion, it is still the case that _most_ tasks involving free text analysis in data science can be carried out using relatively simple methods that we can easily understand without any reference to complex machine learning: methods like bag-of-word models, TFIDF vectors, and (simple n-gram) language models.  It may be that some day soon \"off-the-shelf\" deep learning methods will be so superior to these approaches that there is no need to bother with anything else, but in my experience we are not quite there yet, so this lecture still focuses substantially on the simple approaches, even if it also mentions (without a full derivation) models like word2vec.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free text in data science\n",
    "\n",
    "As mentioned above, the goal that we will consider here of using free text in data science is to extract some meaningful information from the text, _without_ any deep understanding of its meaning.  The reason for this is simple: extracting deep understanding from text is hard.\n",
    "\n",
    "\n",
    "### Natural language processing/understanding\n",
    "The general field of Natural Language Processing (some might differentiate between NLP and \"natural language understanding\", but for our purposes here you can think of these synonymously) looks to truly understand the structure behind free text, e.g. perform tasks like parse the sentences grammatically, describe the general entities and properties that the text is referring to, etc.  But it is easy to come up with examples of why this can be hard.  For example, consider he following example, known as a Winograd schema:\n",
    "\n",
    "> The city councilmen refused the demonstrators a permit because they [feared/advocated] violence.\n",
    "\n",
    "If you read the sentence:\n",
    " \n",
    "> The city councilmen refused the demonstrators a permit because they feared violence.\n",
    "\n",
    "then it is fairly obviously that the \"they\" here refers to the city councilmen; fearing violence would not be a reason for denying someone a permit, so clearly it is the councilmen that fear violence.  On the other hand, in the sentence\n",
    "\n",
    "> The city councilmen refused the demonstrators a permit because they advocated violence.\n",
    "\n",
    "the \"they\" term clearly applies to the demonstrators (the city councilmen presumably would not advocate for violence, let along deny someone a permit because they, the councilmen, were advocating violence).  The point of all this (and this was the original point of these examples, which were originally proposed by Terry Winograd in 1972) is that when we do something \"simple\", like parse a sentence, we bring to bear an enormous amount of outside knowledge and context to this action.  Unlike say, XML documents, there is no internally-specified format that makes language unambiguous; it could be unambiguous only because of external context and knowledge, or it could even be completely ambiguous.  Or in a different vein, there can be grammatically-incorrect sentences that still have clear syntactic meaning, and there can be grammatically-correct sentences that are meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free text in data science\n",
    "\n",
    "Fortunately, for many data science tasks, we can still extract considerable information from text _while only understanding it at an extremely rudimentary level_.  Consider the following two reviews for the same movie:\n",
    "\n",
    "> ... truly, a stunning exercise in large-scale filmmaking; a beautifully-assembled picture in which Abrams combines a magnificent cast with a marvelous flair for big-screen, sci-fi storytelling.\n",
    "\n",
    "and\n",
    "\n",
    "> It's loud and full of vim -- but a little hollow and heartless.\n",
    "\n",
    "Understanding that the first review is positive, while the second review is negative, doesn't take any deep understand of the language itself, it can be done by simple keyword lookup: \"stunning\" and \"marvelous\" are associated with a positive review, while \"hollow\" and \"heartless\" are associated with negative reviews.  Now, of course it's possible to use more complex language to signify a positive review while using some \"negative\" words, with statements like, \"not at all boring\".  But people don't usually write exclusively in this manner (doing so would be \"not at all clear\"), so that the general sentiment of text can still come through very easily even with a few instances where the words themselves can throw you off.\n",
    "\n",
    "With this as context, these notes cover three different examples of methods for extracting some information from free text, without truly understanding much of the language itself.  These methods are becoming standard tools in data science, and have the advantage of being relatively easy to set up and begin processing with.  These methods are:\n",
    "\n",
    "1. Bag of words models and TDIDF vectors\n",
    "2. Word embeddings and word2vec\n",
    "3. N-gram language models\n",
    "\n",
    "**Terminology:** Before we begin, a quick note on terminology.  In these notes \"document\" will mean an individual group of free text data (this could be an actual document or a text field in a database).  \"Words\" or \"terms\" refer to individual tokens separated by whitespace, and additionally also refers to puncutation (so we will often separate punctuation explicitly from the surounding words.  \"Corpus\" refers to a collection of documents, and we will sometimes refer to the set of all _unique_ words/tokens in the corpus as the \"vocabulary\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bag of words models and TFIDF\n",
    "\n",
    "The bag of words model is by far the most common means of representing documents in data science.  Under this model, a document is described soley by the set of words (and possibly their counts) that make up the document.  All information about the actual ordering of the words is ignored.  It is essentially the so-called \"word cloud\" view of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-217a6d563366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwordcloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import wordcloud\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "response = requests.get(\"http://www.datasciencecourse.org\")\n",
    "root = BeautifulSoup(response.content, \"lxml\")\n",
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width=800,height=400).generate(re.sub(r\"\\s+\",\" \", root.text))\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word cloud of text from the Practical Data Science home page.](wordcloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the obvious information we are throwing away with this representation, it send to work surpringingly well in practice, for the precise reason we mentioned above, that the general \"gist\" of many documents can be obtained with only looking at the presence/absence of words in the text.\n",
    "\n",
    "In these notes, we'll cover a simple example of creating so-called TFIDF vectors, which represent the documents via a (weighted) bag of words model. We will then then using these to compute the similarity between documents.  This technique is a common approach for applications like document retrieval and or search, though in the homework you will also experiment using TFIDF vectors as input to a machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency\n",
    "\n",
    "In our example, let's begin with our corpus that contains the following three documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\"the goal of this lecture is to explain the basics of free text processing\",\n",
    "             \"the bag of words model is one such approach\",\n",
    "             \"text processing via bag of words\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setting, we can represent the documents using a _term frequency_ matrix, and $m \\times n$ matrix where $m$ denotes the number of documents, and $n$ denotes the vocabulary size (i.e., the number of unique words across all documents).  To see (the naive way of) how to construct this list, let's first consider a simple way to get a list of all unique words across all documents.  In general there is no need to actually sort the list of words, but we will do so for simplicity here.  It's a good idea to also generate a dictionary that maps words to their index in this list, as we'll frequently want to look up the index corresponding to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['approach', 'bag', 'basics', 'explain', 'free', 'goal', 'is', 'lecture', 'model', 'of', 'one', 'processing', 'such', 'text', 'the', 'this', 'to', 'via', 'words'] \n",
      "\n",
      "{'approach': 0, 'bag': 1, 'basics': 2, 'explain': 3, 'free': 4, 'goal': 5, 'is': 6, 'lecture': 7, 'model': 8, 'of': 9, 'one': 10, 'processing': 11, 'such': 12, 'text': 13, 'the': 14, 'this': 15, 'to': 16, 'via': 17, 'words': 18} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_words = [doc.split() for doc in documents]\n",
    "vocab = sorted(set(sum(document_words, [])))\n",
    "vocab_dict = {k:i for i,k in enumerate(vocab)}\n",
    "print(vocab, \"\\n\")\n",
    "print(vocab_dict, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct a matrix that contains word counts (term frequencies) for all the documents.  We'll also refer to the term frequency of the $j$th word in the $i$th library as $\\mathrm{tf}_{i,j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 1 1 1 0 2 0 1 0 1 2 1 1 0 0]\n",
      " [1 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 0 1]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X_tf = np.zeros((len(documents), len(vocab)), dtype=int)\n",
    "for i,doc in enumerate(document_words):\n",
    "    for word in doc:\n",
    "        X_tf[i, vocab_dict[word]] += 1\n",
    "print(X_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, each of the _rows_ in this matrix correponds to one of the three documents above, and each column correponds to one of the 19 words.  Importantly, note that in practice (for instance on the homework), you will want to create term frequency matrices directly in sparse format, because the term frequency matrix is itself typically sparse (when there are a large number of documents, many words will only be contained in a small number of the document).\n",
    "\n",
    "In this case, we had the entry in our term frequency matrix just correspond to the number of occurences of that term.  But there are other possibilities as well:\n",
    "\n",
    "* The entry can be binary: 1 if the term occurs (any number of times), and 0 otherwise.  This somewhat mitigates the significance of common words that may occur very frequently.\n",
    "* A nonlinear scaling, e.g. $\\log (1+\\mathrm{tf}_{i,j})$, which lies somewhere between the binary case and the raw counts.\n",
    "* A scaled version of term frequencies, e.g., scaling by the maximum term frequency in the document, $\\mathrm{tf}_{i,j} / \\max_k \\mathrm{tf}_{i,k}$.  (Note that this won't affect the actual similarity scores we'll discuss next, because we will ultimately scale each document but it affects the term frequency matrix itself, of course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse document frequency\n",
    "\n",
    "An obvious issue with using normal term frequency counts to represent a document is that the document's vector (and the resulting similarities we will consider) will often be \"dominated\" by very common words, for example: \"of\", \"the\", \"is\", in the preceeding example documents.  This issue can be mitigated to some extent by excluding so-called \"stop words\" (common English words like \"the\", \"a\", \"of\" that aren't considered relevant to the particular documents) from the term frequency matrix.  But this still ignores the case where a word that may not be a generic stop word still happens to occur in a very large number of documents.  Intuitively, we expect that the most \"important\" words in a document are precisely those that only occur in some relatively small number of documents, so that we want to discount the weight of very frequently-occurring terms.\n",
    "\n",
    "This can be accomplished via the inverse document frequency weight for words.  Just as with term frequencies, there are some different weightings of this term, but the most common formulation is\n",
    "\\begin{equation}\n",
    "\\mathrm{idf}_j = \\log\\left(\\frac{\\mbox{# documents}}{\\mbox{# documents with word $j$}}\\right).\n",
    "\\end{equation}\n",
    "As an example, if the word is contained in every document, then the inverse document frequency weight will be zero (log of one).  In contrast, if a word occurs in only one document, its inverse document frequency will be $\\log (\\mbox{# documents})$.\n",
    "\n",
    "Note that inverse document frequency is a _per word_ term, as opposed to term frequency, which is _per word and document_.  We can compute inverse document frequency for our data set as follows, which mainly just requires counting how many documents contain each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.09861229  0.40546511  1.09861229  1.09861229  1.09861229  1.09861229\n",
      "  0.40546511  1.09861229  1.09861229  0.          1.09861229  0.40546511\n",
      "  1.09861229  0.40546511  0.40546511  1.09861229  1.09861229  1.09861229\n",
      "  0.40546511]\n"
     ]
    }
   ],
   "source": [
    "idf = np.log(X_tf.shape[0]/X_tf.astype(bool).sum(axis=0))\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF\n",
    "\n",
    "The term frequency inverse document frequency (TFIDF) combination simply scales the columns of the term frequency matrix by their inverse document frequency.  In doing so, we still have an effective bag of words representation of each document, but we do so with the weighting implied by the inverse document frequency: discouting words that occur very frequently, and increasing the weight of less frequent terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          1.09861229  1.09861229  1.09861229  1.09861229\n",
      "   0.40546511  1.09861229  0.          0.          0.          0.40546511\n",
      "   0.          0.40546511  0.81093022  1.09861229  1.09861229  0.          0.        ]\n",
      " [ 1.09861229  0.40546511  0.          0.          0.          0.\n",
      "   0.40546511  0.          1.09861229  0.          1.09861229  0.\n",
      "   1.09861229  0.          0.40546511  0.          0.          0.\n",
      "   0.40546511]\n",
      " [ 0.          0.40546511  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.40546511  0.\n",
      "   0.40546511  0.          0.          0.          1.09861229  0.40546511]]\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = X_tf * idf\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity\n",
    "\n",
    "Given a TFIDF (or just term frequency) matrix, one of the more common questions to address is to compute similarity between multiple documents in the corpus.  The common metric for doing so is to compute the cosine similarity between two different documents.  This is simply a normalize inner product between the vectors describing each documents.  Specifically,\n",
    "\\begin{equation}\n",
    "\\mbox{CosineSimilarity}(x,y) = \\frac{x^T y}{\\|x\\|_2 \\cdot \\|y\\|_2}.\n",
    "\\end{equation}\n",
    "The cosine similarity is a number between zero (meaning the two documents share no terms in common) and one (meaning the two documents have the exact same term frequency or TFIDF representation).  In fact, the cosine similarity is exactly the converse of the squared Eucliean distance between the normalized document vectors; formally, for $\\tilde{x} = x / \\|x\\|_2$ and $\\tilde{y} = y / \\|y\\|_2$, \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{1}{2}\\|\\tilde{x} - \\tilde{y}\\|_2^2 & = \\frac{1}{2}(\\tilde{x} - \\tilde{y})^T (\\tilde{x} - \\tilde{y}) \\\\\n",
    "& = \\frac{1}{2} (\\tilde{x}^T \\tilde{x} - 2 \\tilde{x}^T \\tilde{y} + \\tilde{y}^T \\tilde{y}) \\\\\n",
    "& = \\frac{1}{2} (1 - 2 \\tilde{x}^T \\tilde{y} + 1) \\\\\n",
    "& = 1 - \\mbox{CosineSimilarity}(x,y).\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "We can compute cosine similarity between the TFIDF vectors in our corpus as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.06796739  0.07771876]\n",
      " [ 0.06796739  1.          0.10281225]\n",
      " [ 0.07771876  0.10281225  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_tfidf_norm = X_tfidf / np.linalg.norm(X_tfidf, axis=1)[:,None]\n",
    "M = X_tfidf_norm @ X_tfidf_norm.T\n",
    "print(M)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
